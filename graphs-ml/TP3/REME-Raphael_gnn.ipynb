{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAN8ZbiRIM9C"
   },
   "source": [
    "# Graph ConvNets in PyTorch\n",
    "\n",
    "PyTorch implementation of the NeurIPS'16 paper:\n",
    "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n",
    "M Defferrard, X Bresson, P Vandergheynst\n",
    "Advances in Neural Information Processing Systems, 3844-3852, 2016\n",
    "[ArXiv preprint](https://arxiv.org/abs/1606.09375)\n",
    "\n",
    "Adapted from Xavier Bresson's repo: [spectral_graph_convnets](https://github.com/xbresson/spectral_graph_convnets) for [dataflowr](https://dataflowr.github.io/website/) by [Marc Lelarge](https://www.di.ens.fr/~lelarge/)\n",
    "\n",
    "## objective:\n",
    "\n",
    "The code provides a simple example of graph ConvNets for the MNIST classification task.\n",
    "The graph is a 8-nearest neighbor graph of a 2D grid.\n",
    "The signals on graph are the MNIST images vectorized as $28^2 \\times 1$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R84-PMawIM9H",
    "outputId": "428eeac4-4bb0-40d6-af6b-612dd1111f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPM7M4H7IM9I"
   },
   "source": [
    "## Download the data\n",
    "\n",
    "If you are running on colab, follow the instructions below.\n",
    "\n",
    "If you cloned the repo, go directly to the tempory hack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJtTTTgeIM9I"
   },
   "source": [
    "### Colab setting\n",
    "\n",
    "If you run this notebook on colab, please uncomment (and run) the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ka4jBE3IM9I",
    "outputId": "fa539842-f74e-4db8-bded-208c64196b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-16 16:12:47--  http://www.di.ens.fr/~lelarge/graphs.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/graphs.tar.gz [following]\n",
      "--2021-03-16 16:12:47--  https://www.di.ens.fr/~lelarge/graphs.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘graphs.tar.gz’\n",
      "\n",
      "graphs.tar.gz           [            <=>     ]  66.42M  17.8MB/s    in 4.9s    \n",
      "\n",
      "2021-03-16 16:12:53 (13.7 MB/s) - ‘graphs.tar.gz’ saved [69647028]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/graphs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pKymA5sIM9J",
    "outputId": "cbea69b2-9534-4a7d-bd43-60ef12bed5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs/\n",
      "graphs/lib/\n",
      "graphs/lib/grid_graph.py\n",
      "graphs/lib/coarsening.py\n",
      "graphs/.spectral_gnn.ipynb.swp\n",
      "graphs/mnist/\n",
      "graphs/mnist/temp/\n",
      "graphs/mnist/temp/MNIST/\n",
      "graphs/mnist/temp/MNIST/raw/\n",
      "graphs/mnist/temp/MNIST/raw/t10k-images-idx3-ubyte\n",
      "graphs/mnist/temp/MNIST/raw/t10k-labels-idx1-ubyte\n",
      "graphs/mnist/temp/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "graphs/mnist/temp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "graphs/mnist/temp/MNIST/raw/train-images-idx3-ubyte\n",
      "graphs/mnist/temp/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "graphs/mnist/temp/MNIST/raw/train-labels-idx1-ubyte\n",
      "graphs/mnist/temp/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "graphs/mnist/temp/MNIST/processed/\n",
      "graphs/mnist/temp/MNIST/processed/test.pt\n",
      "graphs/mnist/temp/MNIST/processed/training.pt\n",
      "graphs/mnist/temp/MNIST.tar.gz\n",
      "graphs/spectral_gnn.ipynb\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf graphs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn2t4_jpIM9K",
    "outputId": "41557ae3-d413-4b75-9c75-a314d0f9af81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/graphs\n"
     ]
    }
   ],
   "source": [
    "%cd graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL9d2fhVIM9K"
   },
   "source": [
    "### temporary hack \n",
    "\n",
    "Unecessary if running on colab (or if you already have MNIST), see this [issue](https://github.com/pytorch/vision/issues/3497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exeFC4D2IM9L"
   },
   "source": [
    "!mkdir mnist\n",
    "%cd mnist\n",
    "!mkdir temp\n",
    "%cd temp\n",
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPLcJokiIM9L"
   },
   "source": [
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3V0ifgWIM9M"
   },
   "source": [
    "%cd ..\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-QfoaerIM9M"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU9R11odIM9M",
    "outputId": "4445d798-e2da-4bc0-b8ba-16acca8ef202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset preprocessing...\n"
     ]
    }
   ],
   "source": [
    "def check_mnist_dataset_exists(path_data='./'):\n",
    "    flag_train_data = os.path.isfile(path_data + 'mnist/train_data.pt') \n",
    "    flag_train_label = os.path.isfile(path_data + 'mnist/train_label.pt') \n",
    "    flag_test_data = os.path.isfile(path_data + 'mnist/test_data.pt') \n",
    "    flag_test_label = os.path.isfile(path_data + 'mnist/test_label.pt') \n",
    "    if flag_train_data==False or flag_train_label==False or flag_test_data==False or flag_test_label==False:\n",
    "        print('MNIST dataset preprocessing...')\n",
    "        import torchvision\n",
    "        import torchvision.transforms as transforms\n",
    "        trainset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=True,\n",
    "                                                download=True, transform=transforms.ToTensor())\n",
    "        testset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=False,\n",
    "                                               download=True, transform=transforms.ToTensor())\n",
    "        train_data=torch.Tensor(60000,28,28)\n",
    "        train_label=torch.LongTensor(60000)\n",
    "        for idx , example in enumerate(trainset):\n",
    "            train_data[idx]=example[0].squeeze()\n",
    "            train_label[idx]=example[1]\n",
    "        torch.save(train_data,path_data + 'mnist/train_data.pt')\n",
    "        torch.save(train_label,path_data + 'mnist/train_label.pt')\n",
    "        test_data=torch.Tensor(10000,28,28)\n",
    "        test_label=torch.LongTensor(10000)\n",
    "        for idx , example in enumerate(testset):\n",
    "            test_data[idx]=example[0].squeeze()\n",
    "            test_label[idx]=example[1]\n",
    "        torch.save(test_data,path_data + 'mnist/test_data.pt')\n",
    "        torch.save(test_label,path_data + 'mnist/test_label.pt')\n",
    "    return path_data\n",
    "\n",
    "\n",
    "_ = check_mnist_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-FvItWZIM9N",
    "outputId": "5c446160-9003-4914-af33-f977d5b1d31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#if you want to play with a small dataset (for cpu), uncomment.\n",
    "#nb_selected_train_data = 500\n",
    "#nb_selected_test_data = 100\n",
    "\n",
    "train_data=torch.load('mnist/train_data.pt').reshape(60000,784).numpy()\n",
    "#train_data = train_data[:nb_selected_train_data,:]\n",
    "print(train_data.shape)\n",
    "\n",
    "train_labels=torch.load('mnist/train_label.pt').numpy()\n",
    "#train_labels = train_labels[:nb_selected_train_data]\n",
    "print(train_labels.shape)\n",
    "\n",
    "test_data=torch.load('mnist/test_data.pt').reshape(10000,784).numpy()\n",
    "#test_data = test_data[:nb_selected_test_data,:]\n",
    "print(test_data.shape)\n",
    "\n",
    "test_labels=torch.load('mnist/test_label.pt').numpy()\n",
    "#test_labels = test_labels[:nb_selected_test_data]\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4d8RsqlIM9O",
    "outputId": "377818b3-b119-434b-beb1-d168ad67a2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges:  6396\n"
     ]
    }
   ],
   "source": [
    "from lib.grid_graph import grid_graph\n",
    "from lib.coarsening import coarsen, HEM, compute_perm, perm_adjacency\n",
    "from lib.coarsening import perm_data\n",
    "\n",
    "# Construct graph\n",
    "t_start = time.time()\n",
    "grid_side = 28\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "\n",
    "\n",
    "######## YOUR GRAPH ADJACENCY MATRIX HERE ########\n",
    "A = grid_graph(grid_side,number_edges,metric) # create graph of Euclidean grid\n",
    "######## YOUR GRAPH ADJACENCY MATRIX HERE ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x1bL7L1JIM9P"
   },
   "outputs": [],
   "source": [
    "def laplacian(W, normalized=True):\n",
    "    \"\"\"Return graph Laplacian\"\"\"\n",
    "    I = scipy.sparse.identity(W.shape[0], dtype=W.dtype)\n",
    "\n",
    "    W += I\n",
    "    # Degree matrix.\n",
    "    d = W.sum(axis=0)\n",
    "\n",
    "    # Laplacian matrix.\n",
    "    if not normalized:\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        L = D - W\n",
    "    else:\n",
    "        #\n",
    "        #\n",
    "        # your code here for normalized laplacian\n",
    "        #\n",
    "        # Using Symmetric normalized Laplacian (eigenvalues <= 2)\n",
    "        D_inv_sqrt = scipy.sparse.diags(1 / np.sqrt(d.A.squeeze()), 0)\n",
    "        L = I - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "\n",
    "    assert np.abs(L - L.T).mean() < 1e-8\n",
    "    assert type(L) is scipy.sparse.csr.csr_matrix\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6PsIUNxsIM9P"
   },
   "outputs": [],
   "source": [
    "def rescale_L(L, lmax=2):\n",
    "    \"\"\"Rescale Laplacian eigenvalues to [-1,1]\"\"\"\n",
    "    M, M = L.shape\n",
    "    I = scipy.sparse.identity(M, format='csr', dtype=L.dtype)\n",
    "    L = 2*L/lmax - I\n",
    "    return L\n",
    "\n",
    "def lmax_L(L):\n",
    "    \"\"\"Compute largest Laplacian eigenvalue\"\"\"\n",
    "    return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSy9EM2KIM9P",
    "outputId": "af26dd03-cc54-454f-eb44-ece5bacad815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy Edge Matching coarsening with Xavier version\n",
      "Layer 0: M_0 = |V| = 960 nodes (176 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 480 nodes (77 added), |E| = 1618 edges\n",
      "Layer 2: M_2 = |V| = 240 nodes (29 added), |E| = 781 edges\n",
      "Layer 3: M_3 = |V| = 120 nodes (7 added), |E| = 388 edges\n",
      "Layer 4: M_4 = |V| = 60 nodes (0 added), |E| = 194 edges\n",
      "lmax: [1.1200929, 1.1981337, 1.1376913, 0.9334328, 0.7780063]\n",
      "Execution time: 1.52s\n"
     ]
    }
   ],
   "source": [
    "normalized = True\n",
    "\n",
    "# Compute coarsened graphs\n",
    "coarsening_levels = 4\n",
    "\n",
    "L, perm = coarsen(A, coarsening_levels, partial(laplacian, normalized=normalized))\n",
    "\n",
    "# Compute max eigenvalue of graph Laplacians\n",
    "lmax = []\n",
    "for i in range(coarsening_levels+1):\n",
    "    lmax.append(lmax_L(L[i]))\n",
    "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels+1)]))\n",
    "\n",
    "# Reindex nodes to satisfy a binary tree structure\n",
    "train_data = perm_data(train_data, perm)\n",
    "test_data = perm_data(test_data, perm)\n",
    "\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "del perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc0SwCcgIM9R"
   },
   "source": [
    "Here, we implemented the pooling layers and computed the list `L` containing the Laplacians of the graphs for each layer.\n",
    "\n",
    "## <font color='red'>Question 1: what is the size of the various poolings?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPsKBfCJIiOX"
   },
   "source": [
    "Each pooling is of size two. We have therefore built the graph for pooling of size 0, 2, 4, 8 and 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY4IypYiIM9R"
   },
   "source": [
    "# Graph ConvNet LeNet5\n",
    "\n",
    "## Layers: CL32-MP4-CL64-MP4-FC512-FC10\n",
    "\n",
    "As described above, this network has 2 graph convolutional layers and two pooling layers with size 4.\n",
    "\n",
    "## <font color='red'>Question 2: which graphs will you take in the list `L` for the graph convolutional layers?</font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTMYxUq7JDhx"
   },
   "source": [
    "First we have to use the original Laplacian L[0].\n",
    "\n",
    "After each max-pooling operation of size 4, we have to use L[i +2] instead of L[i]. We will therefore use L[0], L[2] and L[4] (There are 2 max pooling operations). But as there are no more convolutionnal layers after the last max pooling, L[4] is not needed and only L[0] and L[2] will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH0WoS47I0Zm"
   },
   "source": [
    "In the code below, you will need to complete the `graph_conv_cheby` and the `graph_max_pool`.\n",
    "\n",
    "Hint: each time you permute dimenstions, it is safe to add a `contiguous` like below:\n",
    "`x0 = x.permute(1,2,0).contiguous()` see [here](https://discuss.pytorch.org/t/call-contiguous-after-every-permute-call/13190/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UKiPIVdKIM9R"
   },
   "outputs": [],
   "source": [
    "class Graph_ConvNet_LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "\n",
    "        print('Graph ConvNet: LeNet5')\n",
    "\n",
    "        super(Graph_ConvNet_LeNet5, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        FC1Fin = CL2_F*(D//16)\n",
    "\n",
    "        # graph CL1\n",
    "        self.cl1 = nn.Linear(CL1_K, CL1_F)\n",
    "        self.init_layers(self.cl1, CL1_K, CL1_F)\n",
    "        self.CL1_K = CL1_K; self.CL1_F = CL1_F;\n",
    "\n",
    "        # graph CL2\n",
    "        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F)\n",
    "        self.init_layers(self.cl2, CL2_K*CL1_F, CL2_F)\n",
    "        self.CL2_K = CL2_K; self.CL2_F = CL2_F;\n",
    "\n",
    "        # FC1\n",
    "        self.fc1 = nn.Linear(FC1Fin, FC1_F) \n",
    "        self.init_layers(self.fc1, FC1Fin, FC1_F)\n",
    "        self.FC1Fin = FC1Fin\n",
    "\n",
    "        # FC2\n",
    "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
    "        self.init_layers(self.fc2, FC1_F, FC2_F)\n",
    "\n",
    "        # nb of parameters\n",
    "        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n",
    "        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n",
    "        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n",
    "        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n",
    "        print('nb of parameters=',nb_param,'\\n')\n",
    "        \n",
    "        \n",
    "    def init_layers(self, W, Fin, Fout):\n",
    "\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        W.weight.data.uniform_(-scale, scale)\n",
    "        W.bias.data.fill_(0.0)\n",
    "\n",
    "        return W\n",
    "        \n",
    "        \n",
    "    def graph_conv_cheby(self, x, cl, L, lmax, Fout, K):\n",
    "        # parameters\n",
    "        # B = batch size\n",
    "        # V = nb vertices\n",
    "        # Fin = nb input features\n",
    "        # Fout = nb output features\n",
    "        # K = Chebyshev order & support size\n",
    "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n",
    "\n",
    "        # rescale Laplacian\n",
    "        if normalized:\n",
    "          # L = rescale_L(L)  # = L - I\n",
    "          L = L / 2\n",
    "        else:\n",
    "          # lmax = lmax_L(L)  # Useless\n",
    "          L = rescale_L(L, lmax)\n",
    "\n",
    "        # convert scipy sparse matric L to pytorch\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col)).T \n",
    "        indices = indices.astype(np.int64)\n",
    "        indices = torch.from_numpy(indices)\n",
    "        indices = indices.type(torch.LongTensor)\n",
    "        L_data = L.data.astype(np.float32)\n",
    "        L_data = torch.from_numpy(L_data) \n",
    "        L_data = L_data.type(torch.FloatTensor)\n",
    "        L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n",
    "        L.requires_grad_(False)\n",
    "        if torch.cuda.is_available():\n",
    "            L = L.cuda()\n",
    "\n",
    "        # transform to Chebyshev basis\n",
    "        # Due to limitation of sparse matrix autograd, have to reorder each input for the matrix product.\n",
    "        x_0 = x.permute(1, 0, 2).contiguous()  # V x B x Fin\n",
    "        x_0 = x_0.view(V, B*Fin)  # V x (B*Fin)\n",
    "        x = x_0.unsqueeze(0)  # 1 x V x (B*Fin)\n",
    "\n",
    "        if K > 1:\n",
    "          x_1 = torch.sparse.mm(L, x_0)  # V x (B*Fin)\n",
    "          x = torch.cat((x, x_1.unsqueeze(0)))  # 2 x V x (B*Fin)\n",
    "\n",
    "          for k in range(2, K):\n",
    "              x_2 = 2 * torch.sparse.mm(L, x_1) - x_0  # V x (B*Fin)\n",
    "              x = torch.cat((x, x_2.unsqueeze(0)))  # (k+1) x V x (B*Fin)\n",
    "              x_0, x_1 = x_1, x_2\n",
    "\n",
    "        x = x.view(K, V, B, Fin)  # K x V x B x Fin\n",
    "        x = x.permute(2, 1, 0, 3).contiguous()  # B x V x K x Fin\n",
    "        x = x.view(B, V, K*Fin)  # B x V x K*Fin\n",
    "        x = cl(x)  # B x V x Fout\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Max pooling of size p. Must be a power of 2.\n",
    "    def graph_max_pool(self, x, p): \n",
    "        # \n",
    "        # your code here\n",
    "        # input B x V x F output B x V/p x F\n",
    "        #\n",
    "        max_pool = nn.MaxPool1d(p)\n",
    "        x = max_pool(x.permute(0, 2, 1).contiguous())\n",
    "        return x.permute(0, 2, 1).contiguous()\n",
    "\n",
    "    def forward(self, x, d, L, lmax):\n",
    "        # graph CL1\n",
    "        x = x.unsqueeze(2) # B x V x Fin=1  \n",
    "        x = self.graph_conv_cheby(x, self.cl1, L[0], lmax[0], self.CL1_F, self.CL1_K)\n",
    "        x = F.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        # graph CL2\n",
    "        x = self.graph_conv_cheby(x, self.cl2, L[2], lmax[2], self.CL2_F, self.CL2_K)\n",
    "        x = F.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        # FC1\n",
    "        x = x.view(-1, self.FC1Fin)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x  = nn.Dropout(d)(x)\n",
    "        # FC2\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def loss(self, y, y_target, l2_regularization):\n",
    "    \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            data = param* param\n",
    "            l2_loss += data.sum()\n",
    "           \n",
    "        loss += 0.5* l2_regularization* l2_loss\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr, momentum=0.9 )\n",
    "        \n",
    "        return update\n",
    "        \n",
    "        \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    def evaluation(self, y_predicted, test_l):\n",
    "    \n",
    "        _, class_predicted = torch.max(y_predicted.data, 1)\n",
    "        return 100.0* (class_predicted == test_l).sum()/ y_predicted.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzp6aP9YIM9T",
    "outputId": "e7754dea-da42-4034-e240-a1f2662258a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing network to delete\n",
      "\n",
      "Graph ConvNet: LeNet5\n",
      "nb of parameters= 2023818 \n",
      "\n",
      "Graph_ConvNet_LeNet5(\n",
      "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
      "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=3840, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Delete existing network if exists\n",
    "try:\n",
    "    del net\n",
    "    print('Delete existing network\\n')\n",
    "except NameError:\n",
    "    print('No existing network to delete\\n')\n",
    "\n",
    "# network parameters\n",
    "D = train_data.shape[1]\n",
    "CL1_F = 32\n",
    "CL1_K = 25\n",
    "CL2_F = 64\n",
    "CL2_K = 25\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "dropout_value = 0.5\n",
    "\n",
    "# instantiate the object net of the class \n",
    "net = Graph_ConvNet_LeNet5(net_parameters)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAHvOqwFIM9T"
   },
   "source": [
    "Good time, to check your network is working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUBExiK7IM9U",
    "outputId": "6853fe6f-ea8b-4c19-8201-e12db362247d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data[:5,:], train_labels[:5]\n",
    "train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
    "train_y = train_y.astype(np.int64)\n",
    "train_y = torch.LongTensor(train_y).type(dtypeLong) \n",
    "            \n",
    "# Forward \n",
    "y = net(train_x, dropout_value, L, lmax)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pqSKZucIM9U",
    "outputId": "ddfeb540-2549-43e2-96f2-8e68dd8a1971",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs= 3 , train_size= 60000 , nb_iter= 1800\n",
      "epoch= 1, i=  100, loss(batch)= 0.3953, accuray(batch)= 93.00\n",
      "epoch= 1, i=  200, loss(batch)= 0.3482, accuray(batch)= 94.00\n",
      "epoch= 1, i=  300, loss(batch)= 0.1989, accuray(batch)= 96.00\n",
      "epoch= 1, i=  400, loss(batch)= 0.2671, accuray(batch)= 95.00\n",
      "epoch= 1, i=  500, loss(batch)= 0.2076, accuray(batch)= 95.00\n",
      "epoch= 1, i=  600, loss(batch)= 0.2525, accuray(batch)= 96.00\n",
      "epoch= 1, loss(train)= 0.396, accuracy(train)= 90.837, time= 29.317, lr= 0.05000\n",
      "  accuracy(test) = 97.560 %, time= 3.259\n",
      "epoch= 2, i=  100, loss(batch)= 0.1732, accuray(batch)= 99.00\n",
      "epoch= 2, i=  200, loss(batch)= 0.2309, accuray(batch)= 95.00\n",
      "epoch= 2, i=  300, loss(batch)= 0.2558, accuray(batch)= 96.00\n",
      "epoch= 2, i=  400, loss(batch)= 0.2810, accuray(batch)= 95.00\n",
      "epoch= 2, i=  500, loss(batch)= 0.2127, accuray(batch)= 95.00\n",
      "epoch= 2, i=  600, loss(batch)= 0.1740, accuray(batch)= 97.00\n",
      "epoch= 2, loss(train)= 0.195, accuracy(train)= 97.368, time= 29.140, lr= 0.04750\n",
      "  accuracy(test) = 98.460 %, time= 3.271\n",
      "epoch= 3, i=  100, loss(batch)= 0.2228, accuray(batch)= 97.00\n",
      "epoch= 3, i=  200, loss(batch)= 0.1412, accuray(batch)= 99.00\n",
      "epoch= 3, i=  300, loss(batch)= 0.1725, accuray(batch)= 98.00\n",
      "epoch= 3, i=  400, loss(batch)= 0.1287, accuray(batch)= 99.00\n",
      "epoch= 3, i=  500, loss(batch)= 0.1483, accuray(batch)= 98.00\n",
      "epoch= 3, i=  600, loss(batch)= 0.1434, accuray(batch)= 98.00\n",
      "epoch= 3, loss(train)= 0.163, accuracy(train)= 98.045, time= 29.254, lr= 0.04512\n",
      "  accuracy(test) = 98.730 %, time= 3.282\n"
     ]
    }
   ],
   "source": [
    "# Weights\n",
    "L_net = list(net.parameters())\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "l2_regularization = 5e-4 \n",
    "batch_size = 100\n",
    "num_epochs = 3\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "decay = 0.95\n",
    "decay_steps = train_size\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "# loop over epochs\n",
    "indices = collections.deque()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # reshuffle \n",
    "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    while len(indices) >= batch_size:\n",
    "        \n",
    "        # extract batches\n",
    "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
    "        train_x, train_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
    "        train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
    "        train_y = train_y.astype(np.int64)\n",
    "        train_y = torch.LongTensor(train_y).type(dtypeLong) \n",
    "            \n",
    "        # Forward \n",
    "        y = net(train_x, dropout_value, L, lmax)\n",
    "        loss = net.loss(y,train_y,l2_regularization) \n",
    "        loss_train = loss.detach().item()\n",
    "        # Accuracy\n",
    "        acc_train = net.evaluation(y,train_y.data)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # Update \n",
    "        global_step += batch_size # to update learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # loss, accuracy\n",
    "        running_loss += loss_train\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "        # print        \n",
    "        if not running_total%100: # print every x mini-batches\n",
    "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
    "          \n",
    "    # print \n",
    "    t_stop = time.time() - t_start\n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
    " \n",
    "    # update learning rate \n",
    "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
    "    optimizer = net.update_learning_rate(optimizer, lr)\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    with torch.no_grad():\n",
    "        running_accuray_test = 0\n",
    "        running_total_test = 0\n",
    "        indices_test = collections.deque()\n",
    "        indices_test.extend(range(test_data.shape[0]))\n",
    "        t_start_test = time.time()\n",
    "        while len(indices_test) >= batch_size:\n",
    "            batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
    "            test_x, test_y = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
    "            test_x = torch.FloatTensor(test_x).type(dtypeFloat)\n",
    "            y = net(test_x, 0.0, L, lmax) \n",
    "            test_y = test_y.astype(np.int64)\n",
    "            test_y = torch.LongTensor(test_y).type(dtypeLong)\n",
    "            acc_test = net.evaluation(y,test_y.data)\n",
    "            running_accuray_test += acc_test\n",
    "            running_total_test += 1\n",
    "        t_stop_test = time.time() - t_start_test\n",
    "        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcMxnNIiXLyc"
   },
   "source": [
    "### <font color='red'>Question 3: In this code, each convolutional layer has a parameter K. What does it represent? What are the consequences of choosing a higher or lower value of K? </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1sYCHHfXxEX"
   },
   "source": [
    "K correspond to the number of degree that we will use for our polynomial basis.\n",
    "\n",
    "It also represent the maximum distance of the neighbours that are used to compute the new embedding at each location. (as $L^k_{ij} = 0$ when there is no path of length smaller than k). The filter is K-localized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqHiWQ8iIM9U"
   },
   "source": [
    "### <font color='red'>Question 4: Is it necessary to rescale the Laplacian (in the function `rescale_L`)? Try to remove it and explain what happens. </font> \n",
    "\n",
    "Hint: See Section 2.1 of [the paper](https://arxiv.org/pdf/1606.09375.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsyW8kfGJhQb"
   },
   "source": [
    "Without scaling, we have numerical issues, as the eigenvalues are not scaled, we face exploding computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li12nmcbmhth"
   },
   "source": [
    "### <font color='red'>Question 5: Is it possible to modify the Laplacian to avoid the rescaling step?</font> \n",
    "\n",
    "Hint: Think about the eigenvalues of the Laplacian and how to normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUEqQgHJmjUk"
   },
   "source": [
    "We can use the Symmetric Normalized Laplacian, as it has the property to have its eigenvalues between 0 and 2.\n",
    "\n",
    "Then the rescaling step can just be a division by 2 (or substracting I) which leads to a matrix with the same eigenvectors, but eigenvalues in [0, 1] ([-1, 1] resp.). There is no more needs to compute lmax.\n",
    "\n",
    "Note: Even with the unnormalized laplacian, lmax can be computed out of the model forward step (which was partially done here). And thus it's not a real issue to rescale.\n",
    "\n",
    "The variable `normalized` can be used to parametrize the code in order to use the Symmetric laplacian or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL4bpvypcGBx"
   },
   "source": [
    "### <font color='red'>Question 6: Is GCN like the one presented in video 2 a MGNN? </font> \n",
    "* (A) Yes for K=1\n",
    "* (B) Yes for any value of K\n",
    "* (C) No\n",
    "\n",
    "Explain your answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ktXFqpdgndJ"
   },
   "source": [
    "GCN for K=1 can be seen directly as a MGNN. As it will compute the next features given the feature of the direct neighbours. (as MGNN)\n",
    "\n",
    "But even for greater value of K, a one layer GCN can be seen as a multi layer MGNN. (In a K-layer MGNN, the feature is computed with the information of the K-distant neighbours, as in a K-GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjwfS7bHSTpd"
   },
   "source": [
    "### <font color='red'> Question 7: In which cases do you expect: </font> \n",
    "* a Graph CNN to work better than a CNN?\n",
    "* a CNN to work better than a Graph CNN?\n",
    "\n",
    "For the MNIST classification problem, is there an advantage in using a Graph CNN instead of CNN ? Explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62sZdTlhTb9s"
   },
   "source": [
    "Graph Convolution allows to generalize Convolution layer. On image it seems nonetheless that classical convolution is still better and faster (as shown in the paper).\n",
    "\n",
    "One idea is that there is no notion of up/down with a grid graph (non oriented) representing the image. Which can be a limitation for some images.\n",
    "\n",
    "On more complexe structure such as text, having a Graph Convolution should perform better than classical Convolution as it is able to represent more complex dependencies.\n",
    "\n",
    "On the MNIST classification problem I don't see any advantage of GCN over CNN, as showed in the paper they have slightly worse results, and it's much slower (around 7 times slower). Nonetheless it's useful to show that it works almost as good as CNN in a simple case."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_spectral_gnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
